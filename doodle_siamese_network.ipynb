{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Doodle Recognition with Siamese Network (Image Matching)\n",
        "\n",
        "This notebook implements a **Siamese Network** approach for doodle recognition, which is fundamentally different from traditional classification.\n",
        "\n",
        "## Key Differences from Standard Classification:\n",
        "- **Learns similarity** instead of direct class labels\n",
        "- **Creates embeddings** where similar doodles are close in vector space\n",
        "- **Few-shot learning** - can recognize new categories with just a few examples\n",
        "- **More flexible** - doesn't require retraining for new categories\n",
        "\n",
        "## How It Works:\n",
        "1. Two images are passed through the same network (shared weights)\n",
        "2. The network outputs embeddings for each image\n",
        "3. A contrastive loss pulls similar images together, pushes different ones apart\n",
        "4. At inference, we compare the query image to reference images from each category\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'matplotlib'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import os\n",
        "from PIL import Image\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import json\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# PyTorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    print(\"MPS (Apple Silicon) available: True\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup device (GPU/MPS/CPU)\n",
        "def setup_device():\n",
        "    \"\"\"Setup device for training (CUDA/MPS/CPU)\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device('cuda')\n",
        "        print(f\"✓ Using CUDA: {torch.cuda.get_device_name(0)}\")\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        device = torch.device('mps')\n",
        "        print(\"✓ Using MPS (Apple Silicon)\")\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "        print(\"⚠ Using CPU\")\n",
        "    return device\n",
        "\n",
        "device = setup_device()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "IMG_SIZE = 105  # Standard size for Siamese networks (from original paper)\n",
        "EMBEDDING_DIM = 128  # Dimension of the learned embedding\n",
        "BATCH_SIZE = 32  # Pairs per batch\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_EPOCHS = 30\n",
        "MARGIN = 1.0  # Margin for contrastive loss\n",
        "\n",
        "# Data configuration\n",
        "MAX_CATEGORIES = 50  # Use subset for faster training (None = all)\n",
        "IMAGES_PER_CATEGORY = 500  # Images per category\n",
        "PAIRS_PER_EPOCH = 20000  # Number of pairs to generate per epoch\n",
        "\n",
        "# Few-shot configuration\n",
        "N_WAY = 5  # Number of classes in few-shot test\n",
        "K_SHOT = 5  # Number of support examples per class\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"  Embedding dimension: {EMBEDDING_DIM}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Margin: {MARGIN}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading & Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data paths organized by category\n",
        "def load_data_by_category(base_dir, max_categories=None, max_per_category=None):\n",
        "    \"\"\"\n",
        "    Load image paths organized by category for Siamese network training.\n",
        "    \n",
        "    Returns:\n",
        "        category_images: dict mapping category name -> list of image paths\n",
        "        categories: list of category names\n",
        "    \"\"\"\n",
        "    base_path = Path(base_dir)\n",
        "    all_categories = sorted([d.name for d in base_path.iterdir() if d.is_dir()])\n",
        "    \n",
        "    if max_categories:\n",
        "        categories = all_categories[:max_categories]\n",
        "    else:\n",
        "        categories = all_categories\n",
        "    \n",
        "    print(f\"Loading data from {len(categories)} categories...\")\n",
        "    \n",
        "    category_images = {}\n",
        "    total_images = 0\n",
        "    \n",
        "    for category in tqdm(categories, desc=\"Loading categories\"):\n",
        "        category_path = base_path / category\n",
        "        if not category_path.exists():\n",
        "            continue\n",
        "        \n",
        "        png_files = list(category_path.glob('*.png'))\n",
        "        if max_per_category:\n",
        "            png_files = png_files[:max_per_category]\n",
        "        \n",
        "        # Verify images are valid\n",
        "        valid_files = []\n",
        "        for f in png_files:\n",
        "            try:\n",
        "                with Image.open(f) as img:\n",
        "                    img.verify()\n",
        "                valid_files.append(str(f))\n",
        "            except:\n",
        "                pass\n",
        "        \n",
        "        if len(valid_files) >= 2:  # Need at least 2 images per category for pairs\n",
        "            category_images[category] = valid_files\n",
        "            total_images += len(valid_files)\n",
        "    \n",
        "    print(f\"\\nLoaded {total_images} images from {len(category_images)} categories\")\n",
        "    return category_images, list(category_images.keys())\n",
        "\n",
        "# Load data\n",
        "category_images, categories = load_data_by_category(\n",
        "    'doodles/doodle',\n",
        "    max_categories=MAX_CATEGORIES,\n",
        "    max_per_category=IMAGES_PER_CATEGORY\n",
        ")\n",
        "\n",
        "print(f\"\\nCategories loaded: {categories[:10]}...\")\n",
        "print(f\"Images per category: {[len(category_images[c]) for c in categories[:5]]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample pairs (similar and dissimilar)\n",
        "def visualize_sample_pairs(category_images, categories, num_pairs=4):\n",
        "    \"\"\"Visualize sample similar and dissimilar pairs\"\"\"\n",
        "    fig, axes = plt.subplots(num_pairs, 4, figsize=(12, num_pairs * 3))\n",
        "    \n",
        "    for i in range(num_pairs):\n",
        "        # Similar pair (same category)\n",
        "        cat = random.choice(categories)\n",
        "        img1_path, img2_path = random.sample(category_images[cat], 2)\n",
        "        img1 = Image.open(img1_path)\n",
        "        img2 = Image.open(img2_path)\n",
        "        \n",
        "        axes[i, 0].imshow(img1, cmap='gray')\n",
        "        axes[i, 0].set_title(f'Similar: {cat}')\n",
        "        axes[i, 0].axis('off')\n",
        "        axes[i, 1].imshow(img2, cmap='gray')\n",
        "        axes[i, 1].set_title(f'Similar: {cat}')\n",
        "        axes[i, 1].axis('off')\n",
        "        \n",
        "        # Dissimilar pair (different categories)\n",
        "        cat1, cat2 = random.sample(categories, 2)\n",
        "        img3_path = random.choice(category_images[cat1])\n",
        "        img4_path = random.choice(category_images[cat2])\n",
        "        img3 = Image.open(img3_path)\n",
        "        img4 = Image.open(img4_path)\n",
        "        \n",
        "        axes[i, 2].imshow(img3, cmap='gray')\n",
        "        axes[i, 2].set_title(f'Dissimilar: {cat1}')\n",
        "        axes[i, 2].axis('off')\n",
        "        axes[i, 3].imshow(img4, cmap='gray')\n",
        "        axes[i, 3].set_title(f'Dissimilar: {cat2}')\n",
        "        axes[i, 3].axis('off')\n",
        "    \n",
        "    plt.suptitle('Sample Pairs: Similar (left) vs Dissimilar (right)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_sample_pairs(category_images, categories)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Siamese Dataset & Data Transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "print(\"Transforms defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiamesePairDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset that generates pairs of images for Siamese network training.\n",
        "    \n",
        "    Returns:\n",
        "        (img1, img2, label) where label=1 if same class, label=0 if different\n",
        "    \"\"\"\n",
        "    def __init__(self, category_images, categories, transform=None, pairs_per_epoch=10000):\n",
        "        self.category_images = category_images\n",
        "        self.categories = categories\n",
        "        self.transform = transform\n",
        "        self.pairs_per_epoch = pairs_per_epoch\n",
        "        \n",
        "        # Pre-generate pairs for this epoch\n",
        "        self.pairs = self._generate_pairs()\n",
        "    \n",
        "    def _generate_pairs(self):\n",
        "        \"\"\"Generate balanced pairs of similar and dissimilar images\"\"\"\n",
        "        pairs = []\n",
        "        \n",
        "        # Generate equal number of positive and negative pairs\n",
        "        n_positive = self.pairs_per_epoch // 2\n",
        "        n_negative = self.pairs_per_epoch - n_positive\n",
        "        \n",
        "        # Positive pairs (same category)\n",
        "        for _ in range(n_positive):\n",
        "            cat = random.choice(self.categories)\n",
        "            if len(self.category_images[cat]) >= 2:\n",
        "                img1, img2 = random.sample(self.category_images[cat], 2)\n",
        "                pairs.append((img1, img2, 1))  # label=1 for similar\n",
        "        \n",
        "        # Negative pairs (different categories)\n",
        "        for _ in range(n_negative):\n",
        "            cat1, cat2 = random.sample(self.categories, 2)\n",
        "            img1 = random.choice(self.category_images[cat1])\n",
        "            img2 = random.choice(self.category_images[cat2])\n",
        "            pairs.append((img1, img2, 0))  # label=0 for dissimilar\n",
        "        \n",
        "        random.shuffle(pairs)\n",
        "        return pairs\n",
        "    \n",
        "    def regenerate_pairs(self):\n",
        "        \"\"\"Regenerate pairs for a new epoch\"\"\"\n",
        "        self.pairs = self._generate_pairs()\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img1_path, img2_path, label = self.pairs[idx]\n",
        "        \n",
        "        # Load images as grayscale\n",
        "        img1 = Image.open(img1_path).convert('L')\n",
        "        img2 = Image.open(img2_path).convert('L')\n",
        "        \n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "        \n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "print(\"SiamesePairDataset defined.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and validation sets\n",
        "train_categories, val_categories = train_test_split(\n",
        "    categories, test_size=0.2, random_state=SEED\n",
        ")\n",
        "\n",
        "print(f\"Train categories: {len(train_categories)}\")\n",
        "print(f\"Validation categories: {len(val_categories)}\")\n",
        "print(f\"\\nNote: We split by category to test few-shot generalization!\")\n",
        "\n",
        "# Create category_images dicts for train and val\n",
        "train_category_images = {k: v for k, v in category_images.items() if k in train_categories}\n",
        "val_category_images = {k: v for k, v in category_images.items() if k in val_categories}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = SiamesePairDataset(\n",
        "    train_category_images, train_categories, \n",
        "    transform=train_transform, \n",
        "    pairs_per_epoch=PAIRS_PER_EPOCH\n",
        ")\n",
        "\n",
        "val_dataset = SiamesePairDataset(\n",
        "    val_category_images, val_categories,\n",
        "    transform=val_transform,\n",
        "    pairs_per_epoch=PAIRS_PER_EPOCH // 5\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\nTrain pairs per epoch: {len(train_dataset)}\")\n",
        "print(f\"Validation pairs: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Siamese Network Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SiameseEmbeddingNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding network that produces fixed-size embeddings for images.\n",
        "    Architecture inspired by the original Siamese paper for one-shot learning.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=128):\n",
        "        super(SiameseEmbeddingNet, self).__init__()\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv = nn.Sequential(\n",
        "            # Conv block 1: 1 -> 64 channels\n",
        "            nn.Conv2d(1, 64, kernel_size=10, stride=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Conv block 2: 64 -> 128 channels\n",
        "            nn.Conv2d(64, 128, kernel_size=7, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Conv block 3: 128 -> 128 channels\n",
        "            nn.Conv2d(128, 128, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            # Conv block 4: 128 -> 256 channels\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        \n",
        "        # Calculate the size after conv layers\n",
        "        self._initialize_fc_size()\n",
        "        \n",
        "        # Fully connected layers\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.fc_input_size, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, embedding_dim)\n",
        "        )\n",
        "    \n",
        "    def _initialize_fc_size(self):\n",
        "        \"\"\"Calculate the size of features after conv layers\"\"\"\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, 1, IMG_SIZE, IMG_SIZE)\n",
        "            out = self.conv(dummy)\n",
        "            self.fc_input_size = out.view(1, -1).size(1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Extract embedding from input image\"\"\"\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "        # L2 normalize embeddings\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Full Siamese Network that takes two images and outputs distance/similarity.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=128):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.embedding_net = SiameseEmbeddingNet(embedding_dim)\n",
        "    \n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"Forward pass for pair of images\"\"\"\n",
        "        embedding1 = self.embedding_net(x1)\n",
        "        embedding2 = self.embedding_net(x2)\n",
        "        return embedding1, embedding2\n",
        "    \n",
        "    def get_embedding(self, x):\n",
        "        \"\"\"Get embedding for a single image\"\"\"\n",
        "        return self.embedding_net(x)\n",
        "\n",
        "# Initialize model\n",
        "model = SiameseNetwork(embedding_dim=EMBEDDING_DIM).to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n",
        "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Contrastive Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Contrastive Loss function.\n",
        "    \n",
        "    For similar pairs (label=1): minimize distance\n",
        "    For dissimilar pairs (label=0): maximize distance up to margin\n",
        "    \n",
        "    Loss = (1-Y) * 0.5 * D^2 + Y * 0.5 * max(0, margin - D)^2\n",
        "    \n",
        "    Where:\n",
        "        Y = 1 for similar pairs, 0 for dissimilar\n",
        "        D = euclidean distance between embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "    \n",
        "    def forward(self, embedding1, embedding2, label):\n",
        "        # Euclidean distance\n",
        "        euclidean_distance = F.pairwise_distance(embedding1, embedding2)\n",
        "        \n",
        "        # Contrastive loss\n",
        "        loss_similar = label * torch.pow(euclidean_distance, 2)\n",
        "        loss_dissimilar = (1 - label) * torch.pow(\n",
        "            torch.clamp(self.margin - euclidean_distance, min=0.0), 2\n",
        "        )\n",
        "        \n",
        "        loss = torch.mean(0.5 * (loss_similar + loss_dissimilar))\n",
        "        return loss, euclidean_distance\n",
        "\n",
        "# Initialize loss and optimizer\n",
        "criterion = ContrastiveLoss(margin=MARGIN)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "print(f\"Loss: Contrastive Loss with margin={MARGIN}\")\n",
        "print(f\"Optimizer: Adam with lr={LEARNING_RATE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
